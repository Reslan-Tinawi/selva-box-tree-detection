{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "012b61f5",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Reslan-Tinawi/selva-box-tree-detection/blob/main/notebooks/06_deepforest_pretrained.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e212a116",
      "metadata": {
        "id": "e212a116"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fa0cf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3fa0cf4",
        "outputId": "298c8fd6-0f12-49c1-ce88-712a6c38a885"
      },
      "outputs": [],
      "source": [
        "# detect if running in colab\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    ! pip install torchmetrics\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import random\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import load_from_disk\n",
        "from deepforest import main\n",
        "from PIL.TiffImagePlugin import TiffImageFile\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import v2 as T\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- A100 OPTIMIZATION: ENABLE TF32 ---\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2739ce5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2739ce5",
        "outputId": "9fa6b491-a23c-4ff0-8bcf-d9322077ccf1"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    BASE_PATH = \"/content/drive/MyDrive/datasets/SelvaBox/saved/\"\n",
        "else:\n",
        "    BASE_PATH = \"../data/selvabox/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608af95d",
      "metadata": {
        "id": "608af95d"
      },
      "outputs": [],
      "source": [
        "def setup_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "setup_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57fcb61a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57fcb61a",
        "outputId": "61405965-6032-465a-f4c7-f2da18051c6c"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12cfc353",
      "metadata": {
        "id": "12cfc353"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e4abc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "3767538a7af64246bf3ef23f8f385d9e",
            "4352f14c46ff401d963df654f3097575",
            "58f679e85b124392a9c2c03ecb852ad7",
            "b865746972b84d00801e6176332ddf68",
            "90a51907a3a14870b379853bf85c097b",
            "26e6193f0e0e4729aebfbc9884971ec6",
            "1658d9ef91974f83bb5ebcad1da5bd30",
            "f0e5dd3ad9c3432095d141fbfdc03768",
            "34a932bf66d048c2b507ea92b60e015f",
            "6c4f936f6ddb40339ca5c3158ba912c2",
            "92ef2f387e874a0b828130f10d9f615e",
            "e9e4c52159d641be96ab68fca2c3b03f",
            "41b5f4b669f3485091b2076ff4fe7b96",
            "974a64bdf820435197ba04bebfd7dcc9",
            "4aad43b36e8c496c9ca16565751a2391",
            "675911d47d5e4b2e848f3e3dd479295b",
            "203979e1c52c4be7a5a720f24f33a3ee",
            "43f2665f4fa4422ab4e9c2c421e49828",
            "beb4437ad73f480bb444807b7778c4b4",
            "f0d06b94999b44c58067b2a6f5d9d8ae",
            "49f726358d044504a85b61604257fccd",
            "efaa7b7b0e184a509671c87f67dc09c5"
          ]
        },
        "id": "a6e4abc0",
        "outputId": "9517e934-604e-42bf-e7a8-12ed3f230422"
      },
      "outputs": [],
      "source": [
        "hf_test_ds = load_from_disk(BASE_PATH + \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5236f98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5236f98",
        "outputId": "fe2f5de1-51f2-48b5-9b42-b0b1b5f18009"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of test samples: {len(hf_test_ds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27dc1b0e",
      "metadata": {
        "id": "27dc1b0e"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af8cbe8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def df_to_dict(df):\n",
        "    boxes = df[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].to_numpy()\n",
        "    labels = df[\"label\"].to_numpy()\n",
        "    scores = df[\"score\"].to_numpy()\n",
        "\n",
        "    return {\n",
        "        \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
        "        \"scores\": torch.tensor(scores, dtype=torch.float32),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27637719",
      "metadata": {
        "id": "27637719"
      },
      "outputs": [],
      "source": [
        "def plot_image(\n",
        "    img, boxes, scores=None, labels=None, class_names=None, save_path=None, show=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots bounding boxes on an image with optional scores and labels.\n",
        "\n",
        "    Args:\n",
        "        img (np.array | torch.Tensor): Input image. Shape [H, W, C] (numpy) or [C, H, W] (torch).\n",
        "        boxes (np.array | torch.Tensor): Bounding boxes [N, 4] format (xmin, ymin, xmax, ymax).\n",
        "        scores (np.array | torch.Tensor, optional): Confidence scores [N]. Defaults to None.\n",
        "        labels (np.array | torch.Tensor, optional): Class indices [N]. Defaults to None.\n",
        "        class_names (list, optional): List of class string names. Defaults to None.\n",
        "        save_path (str, optional): Path to save the figure. Defaults to None.\n",
        "        show (bool, optional): Whether to display the plot. Defaults to True.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Data Standardization ---\n",
        "    # Convert PyTorch tensors to Numpy if necessary\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.cpu().numpy()\n",
        "        # If image is [C, H, W], transpose to [H, W, C] for Matplotlib\n",
        "        if img.shape[0] < img.shape[2]:\n",
        "            img = img.transpose(1, 2, 0)\n",
        "\n",
        "    if isinstance(boxes, torch.Tensor):\n",
        "        boxes = boxes.cpu().numpy()\n",
        "\n",
        "    if isinstance(scores, torch.Tensor):\n",
        "        scores = scores.cpu().numpy()\n",
        "\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Normalize image range if it's float 0-1, mostly for display consistency\n",
        "    # (Matplotlib handles 0-1 floats or 0-255 ints, but mixing is bad)\n",
        "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "    # --- 2. Setup Figure ---\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # --- 3. Color Setup ---\n",
        "    # If no class names provided, default to a generic list\n",
        "    if class_names is None:\n",
        "        if labels is not None:\n",
        "            max_label = int(np.max(labels))\n",
        "            class_names = [f\"Class {i}\" for i in range(max_label + 1)]\n",
        "        else:\n",
        "            class_names = [\"Object\"]\n",
        "\n",
        "    # Generate distinct colors for classes\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_names))]\n",
        "\n",
        "    # --- 4. Plotting Loop ---\n",
        "    for i, box in enumerate(boxes):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "\n",
        "        # Determine Label\n",
        "        if labels is not None:\n",
        "            cls_id = int(labels[i])\n",
        "        else:\n",
        "            cls_id = 0  # Default to 0 if no labels provided\n",
        "\n",
        "        color = colors[cls_id % len(colors)]\n",
        "        class_name = (\n",
        "            class_names[cls_id] if cls_id < len(class_names) else f\"Class {cls_id}\"\n",
        "        )\n",
        "\n",
        "        # Draw Rectangle\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "        rect = patches.Rectangle(\n",
        "            (xmin, ymin), width, height, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Build Text String\n",
        "        display_text = class_name\n",
        "        if scores is not None:\n",
        "            display_text += f\" {int(100 * scores[i])}%\"\n",
        "\n",
        "        # Draw Text with background\n",
        "        ax.text(\n",
        "            xmin,\n",
        "            ymin,\n",
        "            display_text,\n",
        "            color=\"white\",\n",
        "            fontsize=10,\n",
        "            verticalalignment=\"top\",\n",
        "            bbox={\n",
        "                \"color\": color,\n",
        "                \"pad\": 2,\n",
        "                \"alpha\": 0.8,\n",
        "            },  # Added alpha for better visibility\n",
        "        )\n",
        "\n",
        "    plt.axis(\"off\")  # Hide axes ticks\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f449d8",
      "metadata": {
        "id": "06f449d8"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab0988d",
      "metadata": {
        "id": "dab0988d"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"project_name\": \"selva-box-tree-detection\",  # WandB project name\n",
        "    \"name\": \"deepforest-pretrained\",\n",
        "    \"num_classes\": 2,  # Background + your classes (e.g., 1 class + 1 background = 2)\n",
        "    \"batch_size\": 16,\n",
        "    \"num_workers\": 4,\n",
        "    \"device\": device,\n",
        "    \"model_name\": \"deepforest_pretrained\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6d755c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "4a6d755c",
        "outputId": "a5727bbc-266d-45cd-b80e-01dbea11fdab"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=CONFIG[\"project_name\"],\n",
        "    name=CONFIG[\"name\"],\n",
        "    config=CONFIG,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120a60b9",
      "metadata": {
        "id": "120a60b9"
      },
      "source": [
        "# Custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75b3b19",
      "metadata": {
        "id": "a75b3b19"
      },
      "outputs": [],
      "source": [
        "# inspired from: https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
        "class SelvaBoxDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, n_classes=1, transforms=None):\n",
        "        self.dataset = hf_dataset\n",
        "        self.n_classes = n_classes\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.dataset[index]\n",
        "        image: TiffImageFile = sample[\"image\"]\n",
        "        annotations_dict = sample[\"annotations\"]\n",
        "\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        # PIL returns (Width, Height)\n",
        "        w, h = image.size\n",
        "\n",
        "        image = tv_tensors.Image(image)\n",
        "\n",
        "        # number of objects/trees in the image\n",
        "        num_objs = len(annotations_dict[\"bbox\"])\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": tv_tensors.BoundingBoxes(\n",
        "                data=annotations_dict[\"bbox\"],\n",
        "                format=\"XYWH\",  # COCO format\n",
        "                canvas_size=(h, w),\n",
        "            ),\n",
        "            \"labels\": torch.zeros(\n",
        "                (num_objs,), dtype=torch.int64\n",
        "            ),  # all trees have label 0\n",
        "            \"image_id\": torch.tensor(\n",
        "                index\n",
        "            ),  # TODO: is this necessary? when moving data to GPU, it expects a tensor\n",
        "            \"area\": torch.tensor(annotations_dict[\"area\"], dtype=torch.float32),\n",
        "            \"iscrowd\": torch.tensor(annotations_dict[\"iscrowd\"], dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        if target[\"boxes\"].shape[0] == 0:\n",
        "            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n",
        "            target[\"area\"] = torch.zeros((0,), dtype=torch.float32)\n",
        "            target[\"iscrowd\"] = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ca6b4e",
      "metadata": {
        "id": "97ca6b4e"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61127843",
      "metadata": {
        "id": "61127843"
      },
      "outputs": [],
      "source": [
        "transforms = T.Compose(\n",
        "    [\n",
        "        T.ConvertBoundingBoxFormat(format=\"XYXY\"),  # Convert COCO format to xyxy\n",
        "        T.ToDtype(torch.float, scale=True),\n",
        "        T.ToPureTensor(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfc9f19",
      "metadata": {
        "id": "cdfc9f19"
      },
      "outputs": [],
      "source": [
        "test_dataset = SelvaBoxDataset(hf_test_ds, transforms=transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912dc664",
      "metadata": {
        "id": "912dc664"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81deeeb",
      "metadata": {
        "id": "b81deeeb"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7845b388",
      "metadata": {
        "id": "7845b388"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    model = main.deepforest()\n",
        "\n",
        "    # Load a pretrained tree detection model from Hugging Face\n",
        "    model.load_model(model_name=\"weecology/deepforest-tree\", revision=\"main\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad30c1e5",
      "metadata": {},
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qsSA-PaO-8qp",
      "metadata": {
        "id": "qsSA-PaO-8qp"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40dbd115",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 52,
          "referenced_widgets": [
            "6487c2a30e284f388def92f845512517",
            "d327088ce36f4f0c968480eb03698b1c",
            "6cfed2052b964db8ba22ff935f8e0f3b",
            "f6185e237e964a708acb1719f9d9e4f2",
            "11f86e62d1e24a9994ec7dc9c0f5d47d",
            "227c62630e434ca29c1cd5d372ebec97",
            "500b83bfe76e4808ba661c234acab7ae",
            "451394faee46459ba468a5e9c3ba0a18",
            "ddacf527c56a44e1a2a6da0be379eddc",
            "927f2adb345c4ee3bacc5cc1723bc373",
            "ac071f1ae6cd49fa9c544683595c0219"
          ]
        },
        "id": "40dbd115",
        "outputId": "9a7d8c73-5b62-4b45-c815-6054eacdeae7"
      },
      "outputs": [],
      "source": [
        "# Initialize the metric\n",
        "metric = MeanAveragePrecision(\n",
        "    box_format=\"xyxy\",\n",
        "    iou_type=\"bbox\",\n",
        "    max_detection_thresholds=[1, 100, 400],\n",
        "    class_metrics=True,\n",
        ")\n",
        "\n",
        "# Assuming test_loader is defined\n",
        "for images, targets in tqdm(test_loader, desc=\"Testing\"):\n",
        "    images = list(image.to(device) for image in images)\n",
        "\n",
        "    # Forward pass\n",
        "    images_batch = torch.stack(images).to(device)\n",
        "    predictions = model.predict_batch(images_batch)\n",
        "\n",
        "    # Move to CPU (torchmetrics handles CPU/GPU, but consistency is good)\n",
        "    predictions = [df_to_dict(pred) for pred in predictions]\n",
        "\n",
        "    # Update the metric with this batch\n",
        "    # targets need to be a list of dicts on the same device as predictions\n",
        "    # If targets are on GPU, move to CPU to match predictions\n",
        "    targets_cpu = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
        "\n",
        "    metric.update(predictions, targets_cpu)\n",
        "\n",
        "    # Clear GPU cache to prevent OOM errors\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Compute the final metrics over the whole dataset\n",
        "results = metric.compute()\n",
        "\n",
        "# Print results\n",
        "print(f\"mAP (IoU=0.50:0.95): {results['map']:.4f}\")\n",
        "print(f\"mAP (IoU=0.50): {results['map_50']:.4f}\")\n",
        "print(f\"mAP (IoU=0.75): {results['map_75']:.4f}\")\n",
        "\n",
        "pprint(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1YtO0ynYDMzA",
      "metadata": {
        "id": "1YtO0ynYDMzA"
      },
      "outputs": [],
      "source": [
        "wandb.log(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FwVVLNO6C8WY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "FwVVLNO6C8WY",
        "outputId": "211bd885-c37a-489c-ff4a-b18f1e6457ff"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aL9QE-V0DPd3",
      "metadata": {
        "id": "aL9QE-V0DPd3"
      },
      "source": [
        "# Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176abd95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize prediction and ground truth on some test images side by side\n",
        "\n",
        "# Get a batch from the test set\n",
        "test_iter = iter(test_loader)\n",
        "images, targets = next(test_iter)\n",
        "\n",
        "images = list(img.to(device) for img in images)\n",
        "\n",
        "# Forward pass\n",
        "images_batch = torch.stack(images).to(device)\n",
        "predictions = model.predict_batch(images_batch)\n",
        "\n",
        "predictions = [df_to_dict(pred) for pred in predictions]\n",
        "\n",
        "images = [img.cpu() for img in images]\n",
        "targets = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
        "\n",
        "for i in range(len(images)):\n",
        "    img = images[i]\n",
        "    pred = predictions[i]\n",
        "    target = targets[i]\n",
        "\n",
        "    # keep only predictions with score > 0.5\n",
        "    keep_idxs = pred[\"scores\"] > 0.5\n",
        "    pred[\"boxes\"] = pred[\"boxes\"][keep_idxs]\n",
        "    pred[\"scores\"] = pred[\"scores\"][keep_idxs]\n",
        "    pred[\"labels\"] = pred[\"labels\"][keep_idxs]\n",
        "\n",
        "    print(f\"Image {i + 1} Predictions:\")\n",
        "    plot_image(\n",
        "        img,\n",
        "        boxes=pred[\"boxes\"],\n",
        "        scores=pred[\"scores\"],\n",
        "        labels=pred[\"labels\"],\n",
        "        class_names=[\"tree\"],\n",
        "        show=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Image {i + 1} Ground Truth:\")\n",
        "    plot_image(\n",
        "        img,\n",
        "        boxes=target[\"boxes\"],\n",
        "        labels=target[\"labels\"],\n",
        "        class_names=[\"tree\"],\n",
        "        show=True,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "selva-box-tree-detection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
